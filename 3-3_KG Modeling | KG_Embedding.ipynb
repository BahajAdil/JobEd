{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHCUINECGYvC",
    "outputId": "de09f882-f1d9-490d-b3ec-7cc8e11eb5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import *\n",
    "import math\n",
    "from torch.utils.data import Sampler, TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import uniform\n",
    "import copy\n",
    "\n",
    "import csv\n",
    "from random import *\n",
    "from collections import defaultdict\n",
    "from os.path import join\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.distributions import uniform\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint\n",
    "\n",
    "KG_PATH = 'data/kg_data'\n",
    "KG_MODEL = 'data/kg_data/trained_models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5q3J5CS6Jt38",
    "outputId": "2bb4c6bf-4ec8-486d-edb0-75f334b46549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "i0G9mWDpGlM7"
   },
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "\n",
    "# torch.manual_seed(1222)\n",
    "# random.seed(159)\n",
    "# np.random.seed(2333)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NVHpoVR8HV7U"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmbHyDIDHZME"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "CexspEzcHXHf"
   },
   "outputs": [],
   "source": [
    "def get_vocab(filename):\n",
    "    word2idx = defaultdict()\n",
    "    with open(filename) as inputfile:\n",
    "        lines = inputfile.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            parts = line.split('\\t')\n",
    "            word2idx[parts[1]] = parts[0]\n",
    "    return word2idx\n",
    "\n",
    "\n",
    "def get_new_model(params):\n",
    "    \n",
    "    if params.model_name == 'BiGumbelBox':\n",
    "        model = BiGumbelBox(params.device, params.VOCAB_SIZE, params.DIM, params.NEG_PER_POS,\n",
    "                            [1e-4, 0.01], [-0.1, -0.001], params).to(params.device)\n",
    "    elif params.model_name == 'QuatE':\n",
    "        model = QuatE(emb_dim=params.DIM, n_entities=params.VOCAB_SIZE,\n",
    "                      n_relations=params.REL_VOCAB_SIZE, ratio = params.NEG_PER_POS, params=params).to(params.device)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_subset_of_given_relations(ids, rel_list):\n",
    "    subs = []\n",
    "    for r in rel_list:\n",
    "        sub = ids[(ids[:, 1] == r).nonzero().squeeze(1)]  # sub triple set\n",
    "        subs.append(sub)\n",
    "    subset = torch.cat(subs, dim=0)\n",
    "    return subset\n",
    "\n",
    "def load_hr_map(data_dir):\n",
    "    file = join(data_dir, 'ndcg_test.pickle')\n",
    "    with open(file, 'rb') as f:\n",
    "        hr_map = pickle.load(f)\n",
    "    return hr_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2MoZyX5HbqF"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "k18p_VssHd3z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Box:\n",
    "    def __init__(self, min_embed, max_embed):\n",
    "        self.min_embed = min_embed\n",
    "        self.max_embed = max_embed\n",
    "        self.delta_embed = max_embed - min_embed\n",
    "    \n",
    "\n",
    "class BiGumbelBox(nn.Module):\n",
    "    def __init__(self, device, vocab_size, embed_dim, ratio, min_init_value, delta_init_value, params):\n",
    "        super(BiGumbelBox, self).__init__()\n",
    "        # super(BiGumbelBox, self).__init__(device, vocab_size, embed_dim, ratio, min_init_value, delta_init_value, params)\n",
    "\n",
    "        self.euler_gamma = 0.57721566490153286060\n",
    "        self.min_init_value = min_init_value\n",
    "        self.delta_init_value = delta_init_value\n",
    "\n",
    "        min_embedding = self.init_embedding(vocab_size, embed_dim, min_init_value)\n",
    "        delta_embedding = self.init_embedding(vocab_size, embed_dim, delta_init_value)\n",
    "        self.min_embedding = nn.Parameter(min_embedding)\n",
    "        self.delta_embedding = nn.Parameter(delta_embedding)\n",
    "\n",
    "        rel_trans_for_head = torch.empty(params.REL_VOCAB_SIZE, params.DIM)\n",
    "        rel_scale_for_head = torch.empty(params.REL_VOCAB_SIZE, params.DIM)\n",
    "        torch.nn.init.normal_(rel_trans_for_head, mean=0, std=1e-4)  # 1e-4 before\n",
    "        torch.nn.init.normal_(rel_scale_for_head, mean=1, std=0.2)  # 0.2 before\n",
    "\n",
    "        rel_trans_for_tail = torch.empty(params.REL_VOCAB_SIZE, params.DIM)\n",
    "        rel_scale_for_tail = torch.empty(params.REL_VOCAB_SIZE, params.DIM)\n",
    "        torch.nn.init.normal_(rel_trans_for_tail, mean=0, std=1e-4)\n",
    "        torch.nn.init.normal_(rel_scale_for_tail, mean=1, std=0.2)\n",
    "\n",
    "        # make nn.Parameter\n",
    "        self.rel_trans_for_head, self.rel_scale_for_head = nn.Parameter(rel_trans_for_head.to(device)), nn.Parameter(\n",
    "            rel_scale_for_head.to(device))\n",
    "        self.rel_trans_for_tail, self.rel_scale_for_tail = nn.Parameter(rel_trans_for_tail.to(device)), nn.Parameter(\n",
    "            rel_scale_for_tail.to(device))\n",
    "\n",
    "        self.true_head, self.true_tail = None, None  # for negative sample filtering\n",
    "        self.gumbel_beta = params.GUMBEL_BETA\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.ratio = ratio\n",
    "        self.vocab_size = vocab_size\n",
    "        self.alpha = 1e-16\n",
    "        self.clamp_min = 0.0\n",
    "        self.clamp_max = 1e10\n",
    "        self.REL_VOCAB_SIZE = params.REL_VOCAB_SIZE\n",
    "\n",
    "\n",
    "    def load_model(self, PATH):\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        model.eval()\n",
    "        \n",
    "    def forward(self, ids, probs, train=True):\n",
    "\n",
    "        head_boxes = self.transform_head_boxes(ids)\n",
    "        tail_boxes = self.transform_tail_boxes(ids)\n",
    "\n",
    "        intersection_boxes = self.intersection(head_boxes, tail_boxes)\n",
    "\n",
    "        log_intersection = self.log_volumes(intersection_boxes)\n",
    "\n",
    "        # condition on subject or object\n",
    "        log_prob = log_intersection - self.log_volumes(tail_boxes)\n",
    "\n",
    "        pos_predictions = log_prob\n",
    "        return pos_predictions, probs\n",
    "\n",
    "\n",
    "    def transform_head_boxes(self, ids):\n",
    "        head_boxes = self.get_entity_boxes(ids[:, 0])\n",
    "\n",
    "        rel_ids = ids[:, 1]\n",
    "        relu = nn.ReLU()\n",
    "\n",
    "        translations = self.rel_trans_for_head[rel_ids]\n",
    "        scales = relu(self.rel_scale_for_head[rel_ids])\n",
    "\n",
    "        # affine transformation\n",
    "        head_boxes.min_embed += translations\n",
    "        head_boxes.delta_embed *= scales\n",
    "        head_boxes.max_embed = head_boxes.min_embed + head_boxes.delta_embed\n",
    "\n",
    "        return head_boxes\n",
    "\n",
    "    def transform_tail_boxes(self, ids):\n",
    "        tail_boxes = self.get_entity_boxes(ids[:, 2])\n",
    "\n",
    "        rel_ids = ids[:, 1]\n",
    "        relu = nn.ReLU()\n",
    "\n",
    "        translations = self.rel_trans_for_tail[rel_ids]\n",
    "        scales = relu(self.rel_scale_for_tail[rel_ids])\n",
    "\n",
    "        # affine transformation\n",
    "        tail_boxes.min_embed += translations\n",
    "        tail_boxes.delta_embed *= scales\n",
    "        tail_boxes.max_embed = tail_boxes.min_embed + tail_boxes.delta_embed\n",
    "\n",
    "        return tail_boxes\n",
    "\n",
    "\n",
    "    def intersection(self, boxes1, boxes2):\n",
    "        intersections_min = self.gumbel_beta * torch.logsumexp(\n",
    "            torch.stack((boxes1.min_embed / self.gumbel_beta, boxes2.min_embed / self.gumbel_beta)),\n",
    "            0\n",
    "        )\n",
    "        intersections_min = torch.max(\n",
    "            intersections_min,\n",
    "            torch.max(boxes1.min_embed, boxes2.min_embed)\n",
    "        )\n",
    "        intersections_max = - self.gumbel_beta * torch.logsumexp(\n",
    "            torch.stack((-boxes1.max_embed / self.gumbel_beta, -boxes2.max_embed / self.gumbel_beta)),\n",
    "            0\n",
    "        )\n",
    "        intersections_max = torch.min(\n",
    "            intersections_max,\n",
    "            torch.min(boxes1.max_embed, boxes2.max_embed)\n",
    "        )\n",
    "\n",
    "        intersection_box = Box(intersections_min, intersections_max)\n",
    "        return intersection_box\n",
    "\n",
    "    def log_volumes(self, boxes, temp=1., gumbel_beta=1., scale=1.):\n",
    "        eps = torch.finfo(boxes.min_embed.dtype).tiny  # type: ignore\n",
    "\n",
    "        if isinstance(scale, float):\n",
    "            s = torch.tensor(scale)\n",
    "        else:\n",
    "            s = scale\n",
    "\n",
    "        log_vol = torch.sum(\n",
    "            torch.log(\n",
    "                F.softplus(boxes.delta_embed - 2 * self.euler_gamma * self.gumbel_beta, beta=temp).clamp_min(eps)\n",
    "            ),\n",
    "            dim=-1\n",
    "        ) + torch.log(s)\n",
    "\n",
    "        return log_vol\n",
    "\n",
    "    def get_entity_boxes(self, entities):\n",
    "        min_rep = self.min_embedding[entities]  # batchsize * embedding_size\n",
    "        delta_rep = self.delta_embedding[entities]\n",
    "        max_rep = min_rep + torch.exp(delta_rep)\n",
    "        boxes = Box(min_rep, max_rep)\n",
    "        return boxes\n",
    "\n",
    "    def init_embedding(self, vocab_size, embed_dim, init_value):\n",
    "        distribution = uniform.Uniform(init_value[0], init_value[1])\n",
    "        box_embed = distribution.sample((vocab_size, embed_dim))\n",
    "        return box_embed\n",
    "\n",
    "    def random_negative_sampling(self, positive_samples, pos_probs, neg_per_pos=None):\n",
    "        if neg_per_pos is None:\n",
    "            neg_per_pos = self.ratio\n",
    "        negative_samples1 = torch.repeat_interleave(positive_samples, neg_per_pos, dim=0)\n",
    "        negative_samples2 = torch.repeat_interleave(positive_samples, neg_per_pos, dim=0)\n",
    "\n",
    "        corrupted_heads = [self.get_negative_samples_for_one_positive(pos, neg_per_pos, mode='corrupt_head') for pos in positive_samples]\n",
    "        corrupted_tails = [self.get_negative_samples_for_one_positive(pos, neg_per_pos, mode='corrupt_tail') for pos in positive_samples]\n",
    "\n",
    "        negative_samples1[:, 0] = torch.cat(corrupted_heads)\n",
    "        negative_samples2[:, 2] = torch.cat(corrupted_tails)\n",
    "        negative_samples = torch.cat((negative_samples1, negative_samples2), 0).to(device)\n",
    "        neg_probs = torch.zeros(negative_samples.shape[0], dtype=pos_probs.dtype).to(device)\n",
    "\n",
    "        return negative_samples, neg_probs\n",
    "\n",
    "    def random_negative_sampling0(self, positive_samples, pos_probs, neg_per_pos=None):\n",
    "        if neg_per_pos is None:\n",
    "            neg_per_pos = self.ratio\n",
    "        negative_samples1 = torch.repeat_interleave(positive_samples, neg_per_pos, dim=0)\n",
    "        negative_samples2 = torch.repeat_interleave(positive_samples, neg_per_pos, dim=0)\n",
    "\n",
    "        # corrupt tails\n",
    "        corrupted_heads = torch.randint(self.vocab_size, (negative_samples1.shape[0],)).to(device)\n",
    "        corrupted_tails = torch.randint(self.vocab_size, (negative_samples1.shape[0],)).to(device)\n",
    "\n",
    "        #filter\n",
    "        bad_heads_idxs = (corrupted_heads == negative_samples1[:,0])\n",
    "        bad_tails_idxs = (corrupted_tails == negative_samples2[:,2])\n",
    "        corrupted_heads[bad_heads_idxs] = torch.randint(self.vocab_size, (torch.sum(bad_heads_idxs),)).to(device)\n",
    "        corrupted_tails[bad_tails_idxs] = torch.randint(self.vocab_size, (torch.sum(bad_tails_idxs),)).to(device)\n",
    "\n",
    "        negative_samples1[:, 0] = corrupted_heads\n",
    "        negative_samples2[:, 2] = corrupted_tails\n",
    "        negative_samples = torch.cat((negative_samples1, negative_samples2), 0).to(device)\n",
    "        neg_probs = torch.zeros(negative_samples.shape[0], dtype=pos_probs.dtype).to(device)\n",
    "\n",
    "        return negative_samples, neg_probs\n",
    "\n",
    "\n",
    "    def get_negative_samples_for_one_positive(self, positive_sample, neg_per_pos, mode):\n",
    "        head, relation, tail = positive_sample\n",
    "        negative_sample_list = []\n",
    "        negative_sample_size = 0\n",
    "        while negative_sample_size < neg_per_pos:\n",
    "            negative_sample = np.random.randint(self.params.VOCAB_SIZE, size=neg_per_pos * 2)\n",
    "\n",
    "            # filter true values\n",
    "            if mode == 'corrupt_head' and (int(relation), int(tail)) in self.true_head:  # filter true heads\n",
    "                # For test data, some (relation, tail) pairs may be unseen and not in self.true_head\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_head[(int(relation), int(tail))],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "                negative_sample = negative_sample[mask]\n",
    "            elif mode == 'corrupt_tail' and (int(head), int(relation)) in self.true_tail:\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_tail[(int(head), int(relation))],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "                negative_sample = negative_sample[mask]\n",
    "            negative_sample_list.append(negative_sample)\n",
    "            negative_sample_size += negative_sample.size\n",
    "\n",
    "        negative_sample = np.concatenate(negative_sample_list)[:neg_per_pos]\n",
    "\n",
    "        negative_sample = torch.from_numpy(negative_sample)\n",
    "        return negative_sample\n",
    "\n",
    "\n",
    "    def head_transformation(self, head_boxes, rel_ids):\n",
    "        relu = nn.ReLU()\n",
    "        translations = self.rel_trans_for_head[rel_ids]\n",
    "        scales = relu(self.rel_scale_for_head[rel_ids])\n",
    "        # affine transformation\n",
    "        head_boxes.min_embed += translations\n",
    "        head_boxes.delta_embed *= scales\n",
    "        head_boxes.max_embed = head_boxes.min_embed + head_boxes.delta_embed\n",
    "\n",
    "        return head_boxes\n",
    "\n",
    "    def tail_transformation(self, tail_boxes, rel_ids):\n",
    "        relu = nn.ReLU()\n",
    "        translations = self.rel_trans_for_tail[rel_ids]\n",
    "        scales = relu(self.rel_scale_for_tail[rel_ids])\n",
    "        # affine transformation\n",
    "        tail_boxes.min_embed += translations\n",
    "        tail_boxes.delta_embed *= scales\n",
    "        tail_boxes.max_embed = tail_boxes.min_embed + tail_boxes.delta_embed\n",
    "\n",
    "        return tail_boxes\n",
    "\n",
    "    def get_entity_boxes_detached(self, entities):\n",
    "        \"\"\"\n",
    "        For logic constraint. We only want to optimize relation parameters, so detach entity parameters\n",
    "        \"\"\"\n",
    "        min_rep = self.min_embedding[entities].detach()\n",
    "        delta_rep = self.delta_embedding[entities].detach()\n",
    "        max_rep = min_rep + torch.exp(delta_rep)\n",
    "        boxes = Box(min_rep, max_rep)\n",
    "        return boxes\n",
    "\n",
    "    def transitive_rule_loss(self, ids):\n",
    "        subsets = [ids[(ids[:,1] == r).nonzero().squeeze(1),:] for r in self.params.RULE_CONFIGS['transitive']['relations']]\n",
    "        sub_ids = torch.cat(subsets, dim=0)\n",
    "\n",
    "        # only optimize relation parameters\n",
    "        head_boxes = self.get_entity_boxes_detached(sub_ids[:, 0])\n",
    "        tail_boxes = self.get_entity_boxes_detached(sub_ids[:, 2])\n",
    "        head_boxes = self.head_transformation(head_boxes, sub_ids[:,1])\n",
    "        tail_boxes = self.tail_transformation(tail_boxes, sub_ids[:,1])\n",
    "\n",
    "        intersection_boxes = self.intersection(head_boxes, tail_boxes)\n",
    "\n",
    "        log_intersection = self.log_volumes(intersection_boxes)\n",
    "\n",
    "        # P(f_r(epsilon_box)|g_r(epsilon_box)) should be 1\n",
    "        vol_loss = torch.norm(1 - torch.exp(log_intersection - self.log_volumes(tail_boxes)))\n",
    "        return vol_loss\n",
    "\n",
    "    def composition_rule_loss(self, ids):\n",
    "        def rels(size, rid):\n",
    "            # fill a tensor with relation id\n",
    "            return torch.full((size,), rid, dtype=torch.long)\n",
    "\n",
    "        def biconditioning(boxes1, boxes2):\n",
    "            intersection_boxes = self.intersection(boxes1, boxes2)\n",
    "            log_intersection = self.log_volumes(intersection_boxes)\n",
    "            # || 1-P(Box2|Box1) ||\n",
    "            condition_on_box1 = torch.norm(1 - torch.exp(log_intersection - self.log_volumes(boxes1)))\n",
    "            # || 1-P(Box1|Box2) ||\n",
    "            condition_on_box2 = torch.norm(1 - torch.exp(log_intersection - self.log_volumes(boxes2)))\n",
    "            loss = condition_on_box1 + condition_on_box2\n",
    "            return loss\n",
    "\n",
    "        vol_loss = 0\n",
    "        for rule_combn in self.params.RULE_CONFIGS['composite']['relations']:\n",
    "            r1, r2, r3 = rule_combn\n",
    "            r1_triples = ids[(ids[:, 1] == r1).nonzero().squeeze(1), :]\n",
    "            r2_triples = ids[(ids[:, 1] == r2).nonzero().squeeze(1), :]\n",
    "\n",
    "            # use heads and tails from r1, r2 as reasonable entity samples to help optimize relation parameters\n",
    "            if len(r1_triples) > 0 and len(r2_triples) > 0:\n",
    "                entities = torch.cartesian_prod(r1_triples[:,0], r2_triples[:,2])\n",
    "                head_ids, tail_ids = entities[:,0], entities[:,1]\n",
    "                size = len(entities)\n",
    "\n",
    "                # only optimize relation parameters\n",
    "                head_boxes_r1r2 = self.get_entity_boxes_detached(head_ids)\n",
    "                tail_boxes_r1r2 = self.get_entity_boxes_detached(tail_ids)\n",
    "                r1r2_head = self.head_transformation(head_boxes_r1r2, rels(size, r1))\n",
    "                r1r2_head = self.head_transformation(r1r2_head, rels(size, r2))\n",
    "                r1r2_tail = self.tail_transformation(tail_boxes_r1r2, rels(size, r1))\n",
    "                r1r2_tail = self.tail_transformation(r1r2_tail, rels(size, r2))\n",
    "\n",
    "                # head_boxes_r1r2 have been modified in transformation\n",
    "                # so make separate box objects with the same parameters\n",
    "                head_boxes_r3 = self.get_entity_boxes_detached(head_ids)\n",
    "                tail_boxes_r3 = self.get_entity_boxes_detached(tail_ids)\n",
    "                r3_head = self.head_transformation(head_boxes_r3, rels(size, r3))\n",
    "                r3_tail = self.tail_transformation(tail_boxes_r3, rels(size, r3))\n",
    "\n",
    "                head_transform_loss = biconditioning(r1r2_head, r3_head)\n",
    "                tail_transform_loss = biconditioning(r1r2_tail, r3_tail)\n",
    "                vol_loss += head_transform_loss\n",
    "                vol_loss += tail_transform_loss\n",
    "        return vol_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdcA3PjMHieV"
   },
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "9zJUisxxHfq7"
   },
   "outputs": [],
   "source": [
    "def get_logic_loss(model, ids, params):\n",
    "    # transitive rule loss regularization\n",
    "    transitive_coff = torch.tensor(params.regularization['transitive']).to(params.device)\n",
    "    if transitive_coff > 0:\n",
    "        transitive_rule_reg = transitive_coff * model.transitive_rule_loss(ids)\n",
    "    else:\n",
    "        transitive_rule_reg = 0\n",
    "\n",
    "    # composite rule loss regularization\n",
    "    composite_coff = torch.tensor(params.regularization['composite']).to(params.device)\n",
    "    if composite_coff > 0:\n",
    "        composition_rule_reg = composite_coff * model.composition_rule_loss(ids)\n",
    "    else:\n",
    "        composition_rule_reg = 0\n",
    "\n",
    "    return (transitive_rule_reg + composition_rule_reg) / len(ids)\n",
    "\n",
    "\n",
    "def main_mse_loss(model, ids, cls):\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    prediction, truth = model(ids, cls, train=True)\n",
    "    mse = criterion(torch.exp(prediction), truth)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def main_msle_loss(model, ids, cls):\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    prediction, truth = model(ids, cls, train=True)\n",
    "    mse = criterion(prediction + 1, torch.log(truth + 1))  # prediction is already log\n",
    "    return mse\n",
    "\n",
    "\n",
    "def main_mle_loss(model, ids, cls):\n",
    "    criterion = nn.L1Loss(reduction='mean')\n",
    "    prediction, truth = model(ids, cls, train=True)\n",
    "    mse = criterion(prediction + 1, torch.log(truth + 1))  # prediction is already log\n",
    "    return mse\n",
    "\n",
    "\n",
    "def L2_regularization(model, ids, params):\n",
    "    regularization = params.regularization\n",
    "    device = params.device\n",
    "    # regularization on delta\n",
    "    delta_coff, min_coff = torch.tensor(regularization['delta']).to(device), torch.tensor(regularization['min']).to(\n",
    "        device)\n",
    "    delta_reg1 = delta_coff * torch.norm(torch.exp(model.delta_embedding[ids[:, 0]]), dim=1).mean()\n",
    "    delta_reg2 = delta_coff * torch.norm(torch.exp(model.delta_embedding[ids[:, 2]]), dim=1).mean()\n",
    "\n",
    "    min_reg1 = min_coff * torch.norm(model.min_embedding[ids[:, 0]], dim=1).mean()\n",
    "    min_reg2 = min_coff * torch.norm(model.min_embedding[ids[:, 2]], dim=1).mean()\n",
    "\n",
    "    rel_trans_coff = torch.tensor(regularization['rel_trans']).to(device)\n",
    "    rel_trans_reg = rel_trans_coff * (\n",
    "            torch.norm(torch.exp(model.rel_trans_for_head[ids[:, 1]]), dim=1).mean() + \\\n",
    "            torch.norm(torch.exp(model.rel_trans_for_tail[ids[:, 1]]), dim=1).mean()\n",
    "    )\n",
    "\n",
    "    rel_scale_coff = torch.tensor(regularization['rel_scale']).to(device)\n",
    "    rel_scale_reg = rel_scale_coff * (\n",
    "            torch.norm(torch.exp(model.rel_scale_for_head[ids[:, 1]]), dim=1).mean() + \\\n",
    "            torch.norm(torch.exp(model.rel_scale_for_tail[ids[:, 1]]), dim=1).mean()\n",
    "    )\n",
    "\n",
    "    L2_reg = delta_reg1 + delta_reg2 + min_reg1 + min_reg2 + rel_trans_reg + rel_scale_reg\n",
    "\n",
    "\n",
    "    return L2_reg\n",
    "\n",
    "def kg_mse_loss(model, ids, cls):\n",
    "    NEG_RATIO = 1\n",
    "    pos_loss = main_mse_loss(model, ids, cls)\n",
    "    negative_samples, neg_probs = model.random_negative_sampling(ids, cls)\n",
    "    neg_loss = main_mse_loss(model, negative_samples, neg_probs)\n",
    "    main_loss = pos_loss + NEG_RATIO * neg_loss\n",
    "    return main_loss, pos_loss, neg_loss\n",
    "\n",
    "def my_loss(model, ids, cls, params):\n",
    "    main_loss, pos_loss, neg_loss = kg_mse_loss(model, ids, cls)\n",
    "\n",
    "    logic_loss = get_logic_loss(model, ids, params)\n",
    "\n",
    "    L2_reg = L2_regularization(model, ids, params)\n",
    "\n",
    "    loss = main_loss + L2_reg + logic_loss\n",
    "\n",
    "    return loss, pos_loss, neg_loss, logic_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHDH34vrHoCl"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "JvnYiP0GHoi3"
   },
   "outputs": [],
   "source": [
    "class TripleDataset(TensorDataset):\n",
    "    \"\"\"Pairwise Probability dataset\"\"\"\n",
    "\n",
    "    def __init__(self, filenames):\n",
    "        read = False\n",
    "        for filename in filenames:\n",
    "            with open(filename, 'rb') as f:\n",
    "                if read:\n",
    "                    temp = pickle.load(f)\n",
    "                    data = np.concatenate((data, temp), axis=0)\n",
    "                else:\n",
    "                    read = True\n",
    "                    data = pickle.load(f)\n",
    "\n",
    "        self.ids = torch.from_numpy(data[:, :3].astype(np.compat.long))\n",
    "        if data.shape[1]>4:\n",
    "            self.probs = torch.from_numpy(data[:, 3:].astype(np.float32))\n",
    "\n",
    "        else:\n",
    "            self.probs = torch.from_numpy(data[:, 3].astype(np.compat.long))\n",
    "        self.length = self.ids.shape[0]\n",
    "        super().__init__(self.ids, self.probs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index], self.probs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "class UncertainTripleDataset(TensorDataset):\n",
    "    def __init__(self, data_dir, filename):\n",
    "        # df = pd.read_csv(join(data_dir, filename), sep='\\t', names=['h', 'r', 't', 'p'])\n",
    "        df = pd.read_csv(join(data_dir, filename))\n",
    "        data = df[['h', 'r', 't']].values\n",
    "\n",
    "        prob = df['p'].values\n",
    "\n",
    "        self.ids = torch.from_numpy(data.astype(np.compat.long)).long()\n",
    "        self.probs = torch.from_numpy(prob.astype(np.float32))\n",
    "        self.length = self.ids.shape[0]\n",
    "        super().__init__(self.ids, self.probs)\n",
    "\n",
    "        self.true_head, self.true_tail = self.get_true_head_and_tail(self.ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ids[index], self.probs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def get_true_head_and_tail(self, triples):\n",
    "        '''\n",
    "        Build a dictionary of true triples that will\n",
    "        be used to filter these true triples for negative sampling\n",
    "        '''\n",
    "\n",
    "        true_head = {}\n",
    "        true_tail = {}\n",
    "\n",
    "        for head0, relation0, tail0 in triples:\n",
    "            head, relation, tail = int(head0), int(relation0), int(tail0)\n",
    "            if (head, relation) not in true_tail:\n",
    "                true_tail[(head, relation)] = []\n",
    "            true_tail[(head, relation)].append(tail)\n",
    "            if (relation, tail) not in true_head:\n",
    "                true_head[(relation, tail)] = []\n",
    "            true_head[(relation, tail)].append(head)\n",
    "\n",
    "        for relation, tail in true_head:\n",
    "            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))\n",
    "        for head, relation in true_tail:\n",
    "            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))\n",
    "\n",
    "        return true_head, true_tail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "oQKhEaKxHr61"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Custom Sampler\n",
    "######################################\n",
    "\n",
    "class TensorBatchSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size, shuffle=False, drop_last=False):\n",
    "        if not isinstance(data_source, TensorDataset):\n",
    "            raise ValueError(f\"data_source should be an instance of torch.utils.data.TensorDataset, but got data_source={data_source}\")\n",
    "        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \\\n",
    "                batch_size <= 0:\n",
    "            raise ValueError(f\"batch_size should be a positive integer value, but got batch_size={batch_size}\")\n",
    "        if not isinstance(shuffle, bool):\n",
    "            raise ValueError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n",
    "        if not isinstance(drop_last, bool):\n",
    "            raise ValueError(f\"drop_last should be a boolean value, but got drop_last={drop_last}\")\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            idxs = torch.randperm(len(self.data_source)).split(self.batch_size)\n",
    "        else:\n",
    "            idxs = torch.arange(len(self.data_source)).split(self.batch_size)\n",
    "        if self.drop_last and len(self.data_source) % self.batch_size != 0:\n",
    "            idxs = idxs[:-1]\n",
    "        return iter(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (math.floor if self.drop_last else math.ceil)(len(self.data_source) / self.batch_size)\n",
    "\n",
    "def unwrap_collate_fn(batch):\n",
    "    return batch[0]\n",
    "\n",
    "class TensorDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "                 batch_sampler=None, *, drop_last=False, collate_fn=None, **kwargs):\n",
    "        if sampler is not None or batch_sampler is not None or collate_fn is not None:\n",
    "            raise ValueError(\"TensorDataLoader does not support alternate samplers, batch samplers, or collate functions.\")\n",
    "        sampler = TensorBatchSampler(dataset, batch_size, shuffle, drop_last)\n",
    "        super().__init__(dataset, batch_size=1, shuffle=False, sampler=sampler,\n",
    "                         drop_last=False, collate_fn = unwrap_collate_fn, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ5Z4FijHuAl"
   },
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QXorLEVrHuhp"
   },
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    class IndexScore:\n",
    "        \"\"\"\n",
    "        The score of a tail when h and r is given.\n",
    "        It's used in the ranking task to facilitate comparison and sorting.\n",
    "        Print w as 3 digit precision float.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, index, score):\n",
    "            self.index = index\n",
    "            self.score = score\n",
    "\n",
    "        def __lt__(self, other):\n",
    "            return self.score < other.score\n",
    "\n",
    "        def __repr__(self):\n",
    "            # return \"(index: %d, w:%.3f)\" % (self.index, self.score)\n",
    "            return \"(%d, %.3f)\" % (self.index, self.score)\n",
    "\n",
    "        def __str__(self):\n",
    "            return \"(index: %d, w:%.3f)\" % (self.index, self.score)\n",
    "\n",
    "    def __init__(self, model, num_entity):\n",
    "        \"\"\"\n",
    "        :type test_dataset: ShirleyTripleDataset\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.num_entity = num_entity\n",
    "\n",
    "    def get_score(self, h, r, i):\n",
    "        ids = torch.LongTensor([[h, r, i]])\n",
    "        cls = torch.Tensor([0])  # dummy\n",
    "        log_score, _ = self.model(ids, cls)\n",
    "        return torch.exp(log_score).detach().cpu().numpy()[0]\n",
    "\n",
    "    def get_t_ranks(self, h, r, ts):\n",
    "        \"\"\"\n",
    "        Given some t index, return the ranks for each t\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ranking_dataset = NDCGRankingTestDataset(\n",
    "            h, r, self.num_entity\n",
    "        )  # for one hr\n",
    "        candidates_data = TensorDataLoader(\n",
    "            ranking_dataset,\n",
    "            batch_size=ranking_dataset.length,\n",
    "            shuffle=False\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for ids in candidates_data:  # only one batch\n",
    "\n",
    "                ids = ids  # [[h,r,0],[h,r,1]...]\n",
    "                cls = torch.zeros(ids.shape[0])\n",
    "                log_scores, _ = self.model(ids, cls)\n",
    "                scores = log_scores\n",
    "                grt_scores = scores[ts]\n",
    "                ranks = np.array([(scores >= s).sum().detach().cpu().numpy() for s in grt_scores])\n",
    "                # print('ranks', ranks)\n",
    "                break\n",
    "\n",
    "        return ranks\n",
    "\n",
    "    def ndcg0(self, h, r, tw_truth):\n",
    "        \"\"\"\n",
    "        Compute nDCG(normalized discounted cummulative gain)\n",
    "        sum(score_ground_truth / log2(rank+1)) / max_possible_dcg\n",
    "        :param tw_truth: [IndexScore1, IndexScore2, ...], soreted by IndexScore.score descending\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # prediction\n",
    "        ts = [tw.index for tw in tw_truth]\n",
    "        ranks = self.get_t_ranks(h, r, ts)\n",
    "\n",
    "        # linear gain\n",
    "        gains = np.array([tw.score for tw in tw_truth])\n",
    "        discounts = np.log2(ranks + 2)  # avoid division by 0\n",
    "        discounted_gains = gains / discounts\n",
    "        dcg = np.sum(discounted_gains)  # discounted cumulative gain\n",
    "\n",
    "        # normalize\n",
    "        best_possible_ranks = np.array([(gains >= g).sum() for g in gains])  # gains [0.9, 0.8, 0.8, 0.7] -> [1,3,3,4]\n",
    "        max_possible_dcg = np.sum(gains / np.log2(best_possible_ranks + 1))\n",
    "        # max_possible_dcg = np.sum(gains / np.log2(np.arange(len(gains)) + 2))\n",
    "\n",
    "        ndcg = dcg / max_possible_dcg  # normalized discounted cumulative gain\n",
    "\n",
    "        # exponential gain\n",
    "        exp_gains = np.array([2 ** tw.score - 1 for tw in tw_truth])\n",
    "        exp_discounted_gains = exp_gains / discounts\n",
    "        exp_dcg = np.sum(exp_discounted_gains)\n",
    "        # normalize\n",
    "        exp_best_possible_ranks = np.array([(exp_gains >= g).sum() for g in exp_gains])\n",
    "        exp_max_possible_dcg = np.sum(exp_gains / np.log2(exp_best_possible_ranks + 1))\n",
    "        # exp_max_possible_dcg = np.sum(exp_gains / np.log2(np.arange(len(gains)) + 2))\n",
    "        exp_ndcg = exp_dcg / exp_max_possible_dcg\n",
    "\n",
    "        return ndcg, exp_ndcg, ranks\n",
    "\n",
    "\n",
    "    def ndcg(self, h, r, tw_truth):\n",
    "        with torch.no_grad():\n",
    "            gains = torch.zeros(self.num_entity)\n",
    "            indices = torch.LongTensor([tw.index for tw in tw_truth])\n",
    "            weights = torch.FloatTensor([tw.score for tw in tw_truth])\n",
    "            gains[indices] = weights\n",
    "\n",
    "            # exp_gains = torch.exp2(gains) - 1\n",
    "\n",
    "            ranking_dataset = NDCGRankingTestDataset(\n",
    "                h, r, self.num_entity\n",
    "            )  # for one hr\n",
    "            candidates_data = TensorDataLoader(\n",
    "                ranking_dataset,\n",
    "                batch_size=ranking_dataset.length,\n",
    "                shuffle=False\n",
    "            )\n",
    "            for ids in candidates_data:  # only one batch\n",
    "\n",
    "                ids = ids  # [[h,r,0],[h,r,1]...]\n",
    "                cls = torch.zeros(ids.shape[0])\n",
    "                log_scores, _ = self.model(ids, cls)\n",
    "                scores = torch.exp(log_scores)\n",
    "                linear_ndcg = ndcg_score(gains.unsqueeze(0).detach().cpu().numpy(), scores.unsqueeze(0).detach().cpu().numpy())\n",
    "                # exp_ndcg = ndcg_score(exp_gains.unsqueeze(0).detach().cpu().numpy(), scores.unsqueeze(0).detach().cpu().numpy())\n",
    "                return linear_ndcg, linear_ndcg, None\n",
    "\n",
    "\n",
    "    def mean_ndcg(self, hr_map):\n",
    "        \"\"\"\n",
    "        :param hr_map: {h:{r:{t:w}}}\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ndcg_sum = 0  # nDCG with linear gain\n",
    "        exp_ndcg_sum = 0\n",
    "        count = 0\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # debug ndcg\n",
    "        res = []  # [(h,r,tw_truth, ndcg)]\n",
    "\n",
    "        for h in hr_map:\n",
    "            for r in hr_map[h]:\n",
    "                tw_dict = hr_map[h][r]  # {t:w}\n",
    "                tw_truth = [self.IndexScore(t, w) for t, w in tw_dict.items()]\n",
    "                tw_truth.sort(reverse=True)  # descending on w\n",
    "                ndcg, exp_ndcg, ranks = self.ndcg(h, r, tw_truth)  # nDCG with linear gain and exponential gain\n",
    "\n",
    "                ndcg_sum += ndcg\n",
    "                exp_ndcg_sum += exp_ndcg\n",
    "                count += 1\n",
    "\n",
    "                # debug\n",
    "                res.append((h, r, tw_truth, ndcg, ranks))\n",
    "\n",
    "        return ndcg_sum / count, exp_ndcg_sum / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "9bMoXJwQHxrN"
   },
   "outputs": [],
   "source": [
    "def evaluate_mse(prediction, truth):\n",
    "    pred = prediction.detach().cpu()\n",
    "    truth_np = truth.detach().cpu().numpy()\n",
    "\n",
    "    mse = (np.square(pred - truth_np)).mean()\n",
    "\n",
    "    mae = (np.absolute(pred - truth_np)).mean()\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "def evaluate_ndcg(model, hr_map, num_entity):\n",
    "    tester = Tester(model, num_entity)\n",
    "    mean_linear_ndcg, mean_exp_ndcg = tester.mean_ndcg(hr_map)\n",
    "    return mean_linear_ndcg, mean_exp_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "R8g-U8ZIHzVP"
   },
   "outputs": [],
   "source": [
    "class NDCGRankingTestDataset(TensorDataset):\n",
    "    def __init__(self, h, r, num_entities):\n",
    "        self.h, self.r = h, r\n",
    "        self.num_entities = num_entities\n",
    "        self.length = num_entities\n",
    "\n",
    "        # make candidate list for ranking task\n",
    "        self.candidate_triples = self.get_all_candidate_triples()\n",
    "\n",
    "    def get_all_candidate_triples(self):\n",
    "        # candidate triples:\n",
    "        # (h, r, 0), (h, r, 1), (h, r, 2) ...\n",
    "        candidates = torch.zeros((self.num_entities, 3), dtype=torch.long)\n",
    "        candidates[:, 0] = self.h\n",
    "        candidates[:, 1] = self.r\n",
    "        candidates[:, 2] = torch.arange(0, self.num_entities)\n",
    "        return candidates\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.candidate_triples[index, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImLuaqstH1tS"
   },
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "6z0XDki7H2GV"
   },
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def set_params(args):\n",
    "    params = Params()\n",
    "    data_name, task, model_name = args.data, args.task, args.model_name\n",
    "    params.data_name = data_name\n",
    "    params.model_name = model_name\n",
    "    if data_name == 'cn15k':\n",
    "        params.VOCAB_SIZE = 15000\n",
    "        params.REL_VOCAB_SIZE = 36\n",
    "    elif data_name == 'nl27k':\n",
    "        params.VOCAB_SIZE = 27221\n",
    "        params.REL_VOCAB_SIZE = 417\n",
    "\n",
    "    params.data_dir = join('./data', data_name)\n",
    "    params.model_dir = join('./trained_models/', data_name)\n",
    "    params.hr_map = load_hr_map(params.data_dir)\n",
    "\n",
    "    params.device = device\n",
    "\n",
    "    if task == 'mse':\n",
    "        params.early_stop = 'valid_mse'  # 'valid_mse' or 'valid_mae' or 'ndcg'\n",
    "    else:\n",
    "        params.early_stop = 'ndcg'\n",
    "\n",
    "    params.whichmodel = 'bigumbelbox'\n",
    "    if data_name == 'cn15k':\n",
    "        if params.early_stop == 'valid_mse' or params.early_stop == 'valid_mae':\n",
    "            params.DIM = 64\n",
    "            params.NEG_PER_POS = 30\n",
    "            \n",
    "            params.EPOCH = 1000\n",
    "            params.BATCH_SIZE = 4096\n",
    "            params.regularization = {'delta': 1, 'min': 1e-3, 'rel_trans': 1e-3, 'rel_scale': 1e-3,\n",
    "                                     'inverse': 0, 'transitive': 0.1, 'composite': 0}  # no composition rule for CN15k\n",
    "            params.GUMBEL_BETA = 0.01  # gumbel box\n",
    "            params.LR = 1e-4\n",
    "            params.NEG_RATIO = 1\n",
    "        elif params.early_stop == 'ndcg':\n",
    "            params.DIM = 300\n",
    "            params.NEG_PER_POS = 30\n",
    "            params.LR = 1e-4\n",
    "            params.EPOCH = 1000\n",
    "            params.BATCH_SIZE = 2048\n",
    "            params.regularization = {'delta': 0.5, 'min': 0, 'rel_trans': 0, 'rel_scale': 0,\n",
    "                                     'inverse': 0, 'transitive': 0.1, 'composite': 0} # no composition rule for CN15k\n",
    "            params.GUMBEL_BETA = 0.001  # gumbel box\n",
    "            params.NEG_RATIO = 1\n",
    "    elif data_name == 'nl27k':\n",
    "        if params.early_stop == 'valid_mse' or params.early_stop == 'valid_mae':\n",
    "            params.DIM = 64\n",
    "            params.NEG_PER_POS = 30\n",
    "            params.LR = 1e-4\n",
    "            params.EPOCH = 1000\n",
    "            params.BATCH_SIZE = 2048\n",
    "            params.regularization = {'delta': 1, 'min': 1e-3, 'rel_trans': 1e-3, 'rel_scale': 1e-3,\n",
    "                                     'inverse': 0, 'transitive': 0.1, 'composite': 0.1}\n",
    "            params.GUMBEL_BETA = 0.01  # gumbel box\n",
    "            params.NEG_RATIO = 1\n",
    "        elif params.early_stop == 'ndcg':\n",
    "            params.DIM = 150\n",
    "            params.NEG_PER_POS = 30\n",
    "            params.LR = 1e-4\n",
    "            params.EPOCH = 1000\n",
    "            params.BATCH_SIZE = 256\n",
    "            params.regularization = {'delta': 0, 'min': 0, 'rel_trans': 0, 'rel_scale': 0,\n",
    "                                     'inverse': 0, 'transitive': 0.1, 'composite': 0.1}\n",
    "            params.GUMBEL_BETA = 0.0001  # gumbel box\n",
    "            params.NEG_RATIO = 1\n",
    "\n",
    "\n",
    "    # define RULE_CONFIGS\n",
    "    if data_name == 'cn15k':\n",
    "        params.RULE_CONFIGS = {\n",
    "            'transitive': { # (a,r,b)^(b,r,c)=>(a,r,c)\n",
    "                'use': True,\n",
    "                'relations': [0, 3, 22],\n",
    "            },\n",
    "        }\n",
    "    elif data_name == 'nl27k':\n",
    "        params.RULE_CONFIGS = {\n",
    "            'transitive': {\n",
    "                'use': True,\n",
    "                'relations': [272, 178, 294],\n",
    "            },\n",
    "            'composite':{\n",
    "                'use': True,\n",
    "                'relations': [(57, 35, 78)],\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Ptv5P4OtH5Bg"
   },
   "outputs": [],
   "source": [
    "def set_general_params(args):\n",
    "    params = Params()\n",
    "    data_name, task, model_name = args.data, args.task, args.model_name\n",
    "    params.data_name = data_name\n",
    "    params.model_name = model_name\n",
    "    \n",
    "    if data_name == 'cn15k':\n",
    "        params.VOCAB_SIZE = 15000\n",
    "        params.REL_VOCAB_SIZE = 36\n",
    "    elif data_name == 'nl27k':\n",
    "        params.VOCAB_SIZE = 27221\n",
    "        params.REL_VOCAB_SIZE = 417\n",
    "\n",
    "    params.data_dir = join('./data', data_name)\n",
    "    params.model_dir = join('./trained_models/', data_name)\n",
    "    params.hr_map = load_hr_map(params.data_dir)\n",
    "    params.device = device\n",
    "    \n",
    "    if task == 'mse':\n",
    "        params.early_stop = 'valid_mse'  # 'valid_mse' or 'valid_mae' or 'ndcg'\n",
    "    else:\n",
    "        params.early_stop = 'ndcg'\n",
    "\n",
    "    params.whichmodel = 'bigumbelbox'\n",
    "    params.DIM = 100\n",
    "    params.NEG_PER_POS = 10\n",
    "    params.LR = 1e-4\n",
    "    params.EPOCH = 1000\n",
    "    params.BATCH_SIZE = 1000\n",
    "    params.regularization = {'delta': 1, 'min': 1e-3, 'rel_trans': 1e-3, 'rel_scale': 1e-3,\n",
    "                             'inverse': 0, 'transitive': 0.1, 'composite': 0}  # no composition rule for CN15k\n",
    "    params.GUMBEL_BETA = 0.01  # gumbel box\n",
    "    params.NEG_RATIO = 1\n",
    "\n",
    "\n",
    "    # define RULE_CONFIGS\n",
    "    if data_name == 'cn15k':\n",
    "        params.RULE_CONFIGS = {\n",
    "            'transitive': { # (a,r,b)^(b,r,c)=>(a,r,c)\n",
    "                'use': True,\n",
    "                'relations': [0, 3, 22],\n",
    "            },\n",
    "        }\n",
    "    elif data_name == 'nl27k':\n",
    "        params.RULE_CONFIGS = {\n",
    "            'transitive': {\n",
    "                'use': True,\n",
    "                'relations': [272, 178, 294],\n",
    "            },\n",
    "            'composite':{\n",
    "                'use': True,\n",
    "                'relations': [(57, 35, 78)],\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "XCL3hIoPzsFK"
   },
   "outputs": [],
   "source": [
    "def kg_params(args):\n",
    "    params = Params()\n",
    "    data_name, task, model_name = args.data, args.task, args.model_name\n",
    "    params.data_name = data_name\n",
    "    params.model_name = model_name\n",
    "\n",
    "    params.VOCAB_SIZE = 20685\n",
    "    params.REL_VOCAB_SIZE = 12\n",
    "\n",
    "    params.data_dir = join(KG_PATH, 'split')\n",
    "    params.model_dir = join(KG_MODEL, 'split')\n",
    "    # params.hr_map = load_hr_map(params.data_dir)\n",
    "    params.device = device\n",
    "    \n",
    "    if task == 'mse':\n",
    "        params.early_stop = 'valid_mse'  # 'valid_mse' or 'valid_mae' or 'ndcg'\n",
    "    else:\n",
    "        params.early_stop = 'ndcg'\n",
    "\n",
    "    params.whichmodel = 'bigumbelbox'\n",
    "    params.DIM = 100\n",
    "    params.NEG_PER_POS = 10\n",
    "    params.LR = 1e-4\n",
    "    params.EPOCH = 1000\n",
    "    params.BATCH_SIZE = 1000\n",
    "    params.regularization = {'delta': 1, 'min': 1e-3, 'rel_trans': 1e-3, 'rel_scale': 1e-3,\n",
    "                             'inverse': 0, 'transitive': 0.1, 'composite': 0}  # no composition rule for CN15k\n",
    "    params.GUMBEL_BETA = 0.01  # gumbel box\n",
    "    params.NEG_RATIO = 1\n",
    "\n",
    "\n",
    "    # define RULE_CONFIGS\n",
    "    if data_name == 'cn15k':\n",
    "        params.RULE_CONFIGS = {\n",
    "            'transitive': { # (a,r,b)^(b,r,c)=>(a,r,c)\n",
    "                'use': True,\n",
    "                'relations': [0, 3, 22],\n",
    "            },\n",
    "        }\n",
    "    elif data_name == 'nl27k':\n",
    "        params.RULE_CONFIGS = {\n",
    "            'transitive': {\n",
    "                'use': True,\n",
    "                'relations': [272, 178, 294],\n",
    "            },\n",
    "            'composite':{\n",
    "                'use': True,\n",
    "                'relations': [(57, 35, 78)],\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3puu3zaKH7u8"
   },
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "xFyCi0CYH5hO"
   },
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "JWZB9T54H9fj"
   },
   "outputs": [],
   "source": [
    "class TrainLoop():\n",
    "    def __init__(self, args):\n",
    "        params = kg_params(args)\n",
    "        # params = set_params(args)\n",
    "        self.params = params\n",
    "        # self.train_dataset = UncertainTripleDataset(params.data_dir, 'train.tsv')\n",
    "        # self.dev_dataset = UncertainTripleDataset(params.data_dir, 'val.tsv')\n",
    "        # self.test_dataset = UncertainTripleDataset(params.data_dir, 'test.tsv')\n",
    "      \n",
    "        self.train_dataset = UncertainTripleDataset(params.data_dir, 'train.csv')\n",
    "        self.dev_dataset = UncertainTripleDataset(params.data_dir, 'val.csv')\n",
    "        self.test_dataset = UncertainTripleDataset(params.data_dir, 'test.csv')\n",
    "        \n",
    "        print(self.params.whichmodel)\n",
    "        print(self.params.early_stop)\n",
    "\n",
    "        if not os.path.exists(self.params.model_dir):\n",
    "            os.makedirs(self.params.model_dir)\n",
    "\n",
    "        self.model = get_new_model(self.params)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.params.LR)\n",
    "\n",
    "    def train_step(self, best_metric):\n",
    "        # Train the model\n",
    "        train_loss = 0\n",
    "        batch_size = self.params.BATCH_SIZE\n",
    "        data = TensorDataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        current_metric = {}\n",
    "        self.model.true_head, self.model.true_tail = self.train_dataset.true_head, self.train_dataset.true_tail  # for negative sampling\n",
    "        for ids, cls in data:\n",
    "            self.model.train()\n",
    "            ids, cls = ids.to(device), cls.to(device)\n",
    "            loss, pos_loss, neg_loss = kg_mse_loss(self.model, ids, cls)\n",
    "#             loss, pos_loss, neg_loss, logic_loss = my_loss(self.model, ids, cls, self.params)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # test at the end of epoch\n",
    "        test_MSE, test_MAE, _, _ = self.test(self.test_dataset, self.params, ndcg_also=False)\n",
    "        # validation\n",
    "        valid_pos_mse, valid_mae, valid_neg_mse, _ = self.test(self.dev_dataset, self.params, neg_mse_also=True)\n",
    "        valid_mse = (valid_pos_mse + self.params.NEG_RATIO * valid_neg_mse) / (1 + self.params.NEG_RATIO)\n",
    "        current_metric['test_mse'] = test_MSE\n",
    "        current_metric['test_mae'] = test_MAE\n",
    "        current_metric['valid_mse'] = valid_mse\n",
    "        current_metric['valid_mae'] = valid_mae\n",
    "\n",
    "        if test_MSE < best_metric['test_mse']:\n",
    "            best_metric['test_mse'] = test_MSE\n",
    "        if test_MAE < best_metric['test_mae']:\n",
    "            best_metric['test_mae'] = test_MAE\n",
    "        if valid_mse < best_metric['valid_mse']:\n",
    "            best_metric['valid_mse'] = valid_mse\n",
    "        if valid_mae < best_metric['valid_mae']:\n",
    "            best_metric['valid_mae'] = valid_mae\n",
    "\n",
    "        return train_loss, best_metric, current_metric\n",
    "    \n",
    "    def eval_step(self, epoch, best_metric, last_best_metric, last_best_epoch):\n",
    "        past_patience = False\n",
    "        res_d = {}\n",
    "        if self.params.early_stop == 'ndcg' and epoch % 10 == 0:\n",
    "            print('####NDCG####')\n",
    "            res_d['linear_ndcg'], res_d['exp_ndcg'] = evaluate_ndcg(self.model, self.params.hr_map, self.params.VOCAB_SIZE)\n",
    "            linear_ndcg = res_d['linear_ndcg']\n",
    "            exp_ndcg = res_d['exp_ndcg']\n",
    "            if linear_ndcg > last_best_ndcg:\n",
    "                last_best_ndcg = linear_ndcg\n",
    "                last_best_epoch = epoch\n",
    "                res_d['best_ndcg'], res_d['exp_ndcg'],res_d['epoch']  = linear_ndcg, exp_ndcg, last_best_epoch\n",
    "                torch.save(self.model, join(self.params.model_dir, f'{self.params.whichmodel}.pt'))\n",
    "            else:\n",
    "                if epoch >= 1 and epoch-last_best_epoch >= 200:\n",
    "                    res_d['best_ndcg'], res_d['exp_ndcg'],res_d['epoch']  = linear_ndcg, exp_ndcg, last_best_epoch\n",
    "                    past_patience = True  # early stop\n",
    "        # early stopping\n",
    "        elif self.params.early_stop == 'valid_mse':\n",
    "            if epoch >= 1 and best_metric['valid_mse'] >= last_best_metric['valid_mse']:  # no improvement or already overfit\n",
    "#                     print('epoch', epoch, 'last_best_epoch', last_best_epoch)\n",
    "                if epoch - last_best_epoch >= 50:  # patience\n",
    "                    res_d['last_best_metric'],res_d['epoch']  = last_best_metric, last_best_epoch\n",
    "                    past_patience = True\n",
    "            else:\n",
    "                last_best_metric = best_metric.copy()\n",
    "                last_best_epoch = epoch\n",
    "\n",
    "                torch.save(self.model, join(self.params.model_dir, f'{self.params.whichmodel}.pt'))\n",
    "\n",
    "        elif self.params.early_stop == 'valid_mae':\n",
    "            if epoch >= 1 and best_metric['valid_mae'] >= last_best_metric['valid_mae']:  # no improvement or already overfit\n",
    "                print('epoch', epoch, 'last_best_epoch', last_best_epoch)\n",
    "                if epoch - last_best_epoch >= 50:  # patience\n",
    "                    res_d['last_best_metric'],res_d['epoch']  = last_best_metric, last_best_epoch\n",
    "                    past_patience = True\n",
    "            else:\n",
    "                last_best_metric = best_metric.copy()\n",
    "                last_best_epoch = epoch\n",
    "                torch.save(self.model, join(self.params.model_dir, f'{self.params.whichmodel}.pt'))\n",
    "        # if past_patience:\n",
    "        #     pprint.pprint(sample_dict)\n",
    "        return last_best_metric, past_patience, last_best_epoch\n",
    "    \n",
    "    def train(self):\n",
    "        best_metric = {\n",
    "            'test_mse': 1,\n",
    "            'valid_mse': 1,\n",
    "            'ndcg': 0,\n",
    "            'test_mae': 100,\n",
    "            'valid_mae': 100\n",
    "        }\n",
    "\n",
    "        last_best_metric = best_metric.copy()\n",
    "        last_best_ndcg = 0\n",
    "        last_best_epoch = 0  # for early stopping\n",
    "        start_time = time.time()\n",
    "        pbar = tqdm(total = self.params.EPOCH)\n",
    "        for epoch in range(self.params.EPOCH):\n",
    "            loss, best_metric, current_metric = self.train_step(best_metric)\n",
    "            last_best_metric, past_patience, last_best_epoch = self.eval_step(epoch, best_metric, last_best_metric, last_best_epoch)\n",
    "            if past_patience:\n",
    "                break\n",
    "            pbar.set_description('E {}| loss {:.2f}| '.format(epoch+1, loss)\\\n",
    "                     +'| '.join(['c_'+k+':{:.4f}'.format(float(current_metric[k])) for k in current_metric])\\\n",
    "                        +'| '\\\n",
    "                     +'| '.join(['b_'+k+':{:.4f}'.format(float(best_metric[k])) for k in best_metric]))\n",
    "            pbar.update()\n",
    "            \n",
    "                    \n",
    "    def test(self, test_data, threshold=None, neg_mse_also=False, ndcg_also=False):\n",
    "        with torch.no_grad():\n",
    "            # return mse, mae, neg_mse, ndcg\n",
    "            # neg also: to test negative samples separately (used for validation)\n",
    "            data = TensorDataLoader(test_data, batch_size=test_data.length)\n",
    "            for ids, cls in data:\n",
    "                ids, cls = ids.to(device), cls.to(device)\n",
    "                prediction, truth = self.model(ids, cls)\n",
    "\n",
    "                score = prediction\n",
    "                label = truth\n",
    "                mse, mae = evaluate_mse(torch.exp(score), label)\n",
    "\n",
    "                ndcg = None\n",
    "                if ndcg_also:\n",
    "                    ndcg = evaluate_ndcg(self.model, self.params.hr_map, self.params.VOCAB_SIZE)\n",
    "\n",
    "                if not neg_mse_also:\n",
    "                    return mse, mae, None, ndcg\n",
    "\n",
    "                # test for negative samples\n",
    "                negative_samples, neg_probs = self.model.random_negative_sampling(ids, cls, neg_per_pos=1)\n",
    "                neg_prediction, _ = self.model(negative_samples, neg_probs)\n",
    "                neg_mse, neg_mae = evaluate_mse(torch.exp(neg_prediction), neg_probs)\n",
    "\n",
    "                combined_mae = (mae+neg_mae)/(1+self.params.NEG_RATIO)  # for validation\n",
    "\n",
    "                return mse, combined_mae, neg_mse, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "JAv-zwP8ID4H"
   },
   "outputs": [],
   "source": [
    "def parse_args(args=None):\n",
    "    args = Params()\n",
    "#     params.model_name BiGumbelBox QuatE\n",
    "#     cn15k or nl27k\n",
    "    args.data = 'cn15k'\n",
    "#     mse or ndcg\n",
    "    args.task = 'mse'\n",
    "    args.model_name = 'BiGumbelBox'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EP_KaSOLIFQ7",
    "outputId": "4433aa49-1501-43ea-b8f4-c0381692c083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigumbelbox\n",
      "valid_mse\n"
     ]
    }
   ],
   "source": [
    "tloop = TrainLoop(parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "5fd413f02aee4cf7ac348dd050c1fddd",
      "6ffcdcfee21e4a88816888c3ea7918ca",
      "a8986e3c8d2148b183533f9963736543",
      "05c19512d5b54563a66dc479e0e75ca5",
      "4a322d8c7ae348c484614a6bb8916c6b",
      "225d9f5622dc464fa8f6fd0b87de01db",
      "d8e588a21b3c4e339b503ff4381012f6",
      "a918b9bec28441848667396de5a47a24",
      "a23ee3bb8bb84fcdb8bdf587ce8d41e0",
      "9fcbc4d903554f988fa2b8977c4cb578",
      "a1702fd2260848fcae4ea6c98d9311da"
     ]
    },
    "id": "xoD-WmJMIGkI",
    "outputId": "0f063400-35e9-4865-c22a-793d02493691"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd413f02aee4cf7ac348dd050c1fddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tloop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4LSqKjP2kgv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c19512d5b54563a66dc479e0e75ca5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fcbc4d903554f988fa2b8977c4cb578",
      "placeholder": "​",
      "style": "IPY_MODEL_a1702fd2260848fcae4ea6c98d9311da",
      "value": " 31/1000 [3:09:57&lt;98:03:20, 364.29s/it]"
     }
    },
    "225d9f5622dc464fa8f6fd0b87de01db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a322d8c7ae348c484614a6bb8916c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd413f02aee4cf7ac348dd050c1fddd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ffcdcfee21e4a88816888c3ea7918ca",
       "IPY_MODEL_a8986e3c8d2148b183533f9963736543",
       "IPY_MODEL_05c19512d5b54563a66dc479e0e75ca5"
      ],
      "layout": "IPY_MODEL_4a322d8c7ae348c484614a6bb8916c6b"
     }
    },
    "6ffcdcfee21e4a88816888c3ea7918ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_225d9f5622dc464fa8f6fd0b87de01db",
      "placeholder": "​",
      "style": "IPY_MODEL_d8e588a21b3c4e339b503ff4381012f6",
      "value": "E 31| loss 0.60| c_test_mse:0.0002| c_test_mae:0.0032| c_valid_mse:0.0001| c_valid_mae:0.0016| b_test_mse:0.0001| b_valid_mse:0.0001| b_ndcg:0.0000| b_test_mae:0.0024| b_valid_mae:0.0015:   3%"
     }
    },
    "9fcbc4d903554f988fa2b8977c4cb578": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1702fd2260848fcae4ea6c98d9311da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a23ee3bb8bb84fcdb8bdf587ce8d41e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a8986e3c8d2148b183533f9963736543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a918b9bec28441848667396de5a47a24",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a23ee3bb8bb84fcdb8bdf587ce8d41e0",
      "value": 31
     }
    },
    "a918b9bec28441848667396de5a47a24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8e588a21b3c4e339b503ff4381012f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
